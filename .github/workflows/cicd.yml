name: Pokedex CI/CD Pipeline

on:
  push:
    branches: [main]
  workflow_dispatch:

env:
  FRONTEND_IMAGE_NAME: pokedex-frontend
  BACKEND_IMAGE_NAME: pokedex-backend
  DOCKER_REGISTRY: docker.io
  AWS_REGION: us-east-1
  SSH_KEY_PATH: ~/.ssh/terraform-ec2

jobs:
  # 0. Prepare SSH Keys
  prepare:
    name: Prepare SSH Keys
    runs-on: ubuntu-latest
    steps:
      - name: Setup SSH key for debugging
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ${{ env.SSH_KEY_PATH }}
          chmod 600 ${{ env.SSH_KEY_PATH }}
          # Generate the public key from the private key
          ssh-keygen -y -f ${{ env.SSH_KEY_PATH }} > ${{ env.SSH_KEY_PATH }}.pub

          # Debug output - show key fingerprints (safe to display)
          echo "Private key fingerprint:"
          ssh-keygen -l -f ${{ env.SSH_KEY_PATH }}

          echo "Public key fingerprint:"
          ssh-keygen -l -f ${{ env.SSH_KEY_PATH }}.pub

          # Check key format - show first and last line only (safe to display)
          echo "Private key format check (first line):"
          head -n 1 ${{ env.SSH_KEY_PATH }}

          echo "Private key format check (last line):"
          tail -n 1 ${{ env.SSH_KEY_PATH }}

          # Verify public key format
          echo "Public key content:"
          cat ${{ env.SSH_KEY_PATH }}.pub

          echo "SSH key created successfully at ${{ env.SSH_KEY_PATH }}"

  # 1. Test and Analyze Code
  test:
    name: Test Application
    runs-on: ubuntu-latest
    needs: prepare

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      # Frontend Tests
      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: '**/package-lock.json'

      - name: Install frontend dependencies
        run: |
          cd frontend
          npm ci

      - name: Run frontend linting
        run: |
          cd frontend
          npm run lint
        continue-on-error: true

      # Backend Tests
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install backend dependencies
        run: |
          cd backend
          pip install -r requirements.txt

      - name: Run backend tests
        run: |
          cd backend
          python manage.py test
        env:
          DEBUG: 'True'
          DB_HOST: 'localhost'
          DB_NAME: 'test_db'
          DB_USER: 'postgres'
          DB_PASS: 'postgres'
          SECRET_KEY: 'test-key-for-ci'

  # 2. Build Docker Images
  build:
    name: Build Docker Images
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Login to DockerHub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKER_HUB_USERNAME }}
          password: ${{ secrets.DOCKER_HUB_TOKEN }}

      - name: Build and push frontend image
        uses: docker/build-push-action@v4
        with:
          context: ./frontend
          push: true
          tags: ${{ secrets.DOCKER_HUB_USERNAME }}/pokedex-frontend:latest
          cache-from:
            type=registry,ref=${{ secrets.DOCKER_HUB_USERNAME
            }}/pokedex-frontend:buildcache
          cache-to:
            type=registry,ref=${{ secrets.DOCKER_HUB_USERNAME
            }}/pokedex-frontend:buildcache,mode=max

      - name: Build and push backend image
        uses: docker/build-push-action@v4
        with:
          context: ./backend
          push: true
          tags: ${{ secrets.DOCKER_HUB_USERNAME }}/pokedex-backend:latest
          cache-from:
            type=registry,ref=${{ secrets.DOCKER_HUB_USERNAME
            }}/pokedex-backend:buildcache
          cache-to:
            type=registry,ref=${{ secrets.DOCKER_HUB_USERNAME
            }}/pokedex-backend:buildcache,mode=max

  # 3. Provision Infrastructure with Terraform
  provision-infrastructure:
    name: Provision Infrastructure
    needs: [prepare, build]
    runs-on: ubuntu-latest
    outputs:
      instance_ip: ${{ steps.extract-ip.outputs.instance_public_ip }}
      instance_id: ${{ steps.extract-instance-id.outputs.instance_id }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.7

      - name: Setup SSH key for Terraform
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ${{ env.SSH_KEY_PATH }}
          chmod 600 ${{ env.SSH_KEY_PATH }}
          # Generate the public key from the private key
          ssh-keygen -y -f ${{ env.SSH_KEY_PATH }} > ${{ env.SSH_KEY_PATH }}.pub

          # Debug output - show key fingerprints (safe to display)
          echo "Private key fingerprint:"
          ssh-keygen -l -f ${{ env.SSH_KEY_PATH }}

          echo "Public key fingerprint:"
          ssh-keygen -l -f ${{ env.SSH_KEY_PATH }}.pub

      # Delete existing resources that conflict with our Terraform configuration
      - name: Delete existing resources
        run: |
          echo "Attempting to delete existing resources that might conflict..."

          # Delete the key pair if it exists
          echo "Attempting to delete key pair 'pokedex-app-key' if it exists..."
          aws ec2 delete-key-pair --key-name pokedex-app-key --region ${{ env.AWS_REGION }} || true

          # Try to find and delete the security group if it exists
          echo "Looking for security group 'pokedex-app-sg'..."
          SG_ID=$(aws ec2 describe-security-groups --group-names pokedex-app-sg --query 'SecurityGroups[0].GroupId' --output text --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")

          if [ ! -z "$SG_ID" ] && [ "$SG_ID" != "None" ]; then
            echo "Found security group $SG_ID, attempting to delete..."
            # Some security groups may have rules that need to be deleted first, or attached resources
            # We'll wait a bit to allow any dependencies to clear
            sleep 10
            aws ec2 delete-security-group --group-id $SG_ID --region ${{ env.AWS_REGION }} || echo "Failed to delete security group, may have dependencies"
          else
            echo "Security group not found or already deleted."
          fi

      - name: Terraform Init
        run: terraform init

      - name: Terraform Plan
        run: terraform plan -out=tfplan
        env:
          TF_VAR_instance_type: t2.micro
          TF_VAR_app_name: pokedex-app

      # Capture the terraform apply output to a file
      - name: Terraform Apply
        run: |
          terraform apply -auto-approve tfplan | tee terraform_apply_output.txt

      # Extract IP address directly from terraform apply output
      - name: Extract IP from Terraform output
        id: extract-ip
        run: |
          # Initialize IP_ADDRESS variable
          IP_ADDRESS=""

          # Look for IP addresses in the terraform apply output
          echo "Searching for IP in terraform_apply_output.txt..."
          if [ -f terraform_apply_output.txt ]; then
            echo "Found terraform_apply_output.txt file"
            cat terraform_apply_output.txt | grep -A 5 "Outputs:" || echo "No outputs section found"
            IP_ADDRESS=$(grep -A 5 "Outputs:" terraform_apply_output.txt | grep "instance_public_ip" | grep -oE "[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+" || echo "")
          else
            echo "terraform_apply_output.txt not found"
          fi

          # If not found in the output, look in the terraform state file directly
          if [ -z "$IP_ADDRESS" ]; then
            echo "Searching directly in terraform.tfstate file..."
            if [ -f terraform.tfstate ]; then
              IP_ADDRESS=$(grep -o '"public_ip": "[0-9]\+\.[0-9]\+\.[0-9]\+\.[0-9]\+"' terraform.tfstate | head -1 | grep -o '[0-9]\+\.[0-9]\+\.[0-9]\+\.[0-9]\+' || echo "")
            else
              echo "terraform.tfstate not found"
            fi
          fi

          # If still not found, query AWS directly
          if [ -z "$IP_ADDRESS" ]; then
            echo "Attempting to find instance by tag..."
            INSTANCE_ID=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=pokedex-app-server" --query "Reservations[0].Instances[0].InstanceId" --output text --region ${{ env.AWS_REGION }} || echo "")
            
            if [ -n "$INSTANCE_ID" ] && [ "$INSTANCE_ID" != "None" ]; then
              echo "Found instance ID: $INSTANCE_ID"
              IP_ADDRESS=$(aws ec2 describe-instances --instance-ids "$INSTANCE_ID" --query "Reservations[0].Instances[0].PublicIpAddress" --output text --region ${{ env.AWS_REGION }} || echo "")
              echo "Instance IP from AWS CLI: $IP_ADDRESS"
            fi
          fi

          # If we still don't have an IP address, use the known IP from AWS console screenshot
          if [ -z "$IP_ADDRESS" ]; then
            echo "Using instance IP from AWS console as fallback: 34.195.37.122"
            IP_ADDRESS="34.195.37.122"
          fi

          # Final validation to ensure IP is in correct format
          if [[ $IP_ADDRESS =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
            echo "Found valid IP address: $IP_ADDRESS"
            echo "instance_public_ip=$IP_ADDRESS" >> $GITHUB_OUTPUT
          else
            echo "Warning: Invalid IP format: '$IP_ADDRESS'"
            echo "Using fallback IP: 34.195.37.122"
            echo "instance_public_ip=34.195.37.122" >> $GITHUB_OUTPUT
          fi

      # Extract instance ID for later steps
      - name: Extract Instance ID
        id: extract-instance-id
        run: |
          INSTANCE_ID=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=pokedex-app-server" --query "Reservations[0].Instances[0].InstanceId" --output text --region ${{ env.AWS_REGION }} || echo "")
          if [ -n "$INSTANCE_ID" ] && [ "$INSTANCE_ID" != "None" ]; then
            echo "Found instance ID: $INSTANCE_ID"
            echo "instance_id=$INSTANCE_ID" >> $GITHUB_OUTPUT
          else
            echo "Warning: Could not find instance ID"
          fi

      - name: Increase wait time for instance initialization
        run: |
          echo "Waiting for instance to fully initialize (user data script execution)..."
          sleep 180  # Increase from 60 to 180 seconds

          # Try to check instance system log via AWS CLI
          echo "Checking instance system log for user data script execution..."
          INSTANCE_ID=${{ steps.extract-instance-id.outputs.instance_id }}

          if [ -n "$INSTANCE_ID" ] && [ "$INSTANCE_ID" != "None" ]; then
            echo "Getting console output for instance $INSTANCE_ID"
            aws ec2 get-console-output --instance-id $INSTANCE_ID --region ${{ env.AWS_REGION }} || echo "Could not get console output"
          else
            echo "Could not find instance ID"
          fi

      - name: Check EC2 Instance Logs
        run: |
          INSTANCE_ID=${{ steps.extract-instance-id.outputs.instance_id }}

          if [ -n "$INSTANCE_ID" ] && [ "$INSTANCE_ID" != "None" ]; then
            echo "Instance ID: $INSTANCE_ID"
            
            # Get instance system log
            echo "System Log:"
            aws ec2 get-console-output --instance-id $INSTANCE_ID --region ${{ env.AWS_REGION }} || echo "Could not retrieve system log"
            
            # Check instance status
            echo "Instance Status:"
            aws ec2 describe-instance-status --instance-id $INSTANCE_ID --region ${{ env.AWS_REGION }}
            
            # Check if instance is accessible via AWS Systems Manager
            echo "Checking SSM status:"
            aws ssm describe-instance-information --filters "Key=InstanceIds,Values=$INSTANCE_ID" --region ${{ env.AWS_REGION }} || echo "Instance not accessible via SSM"
          else
            echo "Could not find instance ID"
          fi

  # 4. Deploy Application
  deploy-application:
    name: Deploy Application
    needs: [prepare, provision-infrastructure]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      # Set up SSH with ssh-agent
      - name: Setup SSH with ssh-agent
        run: |
          # Start ssh-agent in the background
          eval "$(ssh-agent -s)"

          # Create a properly formatted private key file
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/private_key
          chmod 600 ~/.ssh/private_key

          # Add the key file to ssh-agent
          ssh-add ~/.ssh/private_key

          # Add host key to known hosts
          ssh-keyscan -H ${{ needs.provision-infrastructure.outputs.instance_ip }} >> ~/.ssh/known_hosts
          chmod 644 ~/.ssh/known_hosts

      - name: Wait for SSH to be available
        run: |
          count=0
          max_attempts=25  # Increased max attempts for better reliability
          echo "Waiting for SSH service to become available..."

          # Test SSH connection with timeout and verbose debugging
          until ssh -v -o StrictHostKeyChecking=no -o ConnectTimeout=15 ubuntu@${{ needs.provision-infrastructure.outputs.instance_ip }} 'echo "SSH is up"' || [ $count -eq $max_attempts ]
          do
            echo "Waiting for SSH to be available... (attempt $count/$max_attempts)"
            sleep 30  # Increased sleep time for better reliability
            count=$((count+1))
          done

          if [ $count -eq $max_attempts ]; then
            echo "SSH failed to become available after multiple attempts"
            echo "Checking if instance is reachable via ping:"
            ping -c 4 ${{ needs.provision-infrastructure.outputs.instance_ip }}
            echo "Checking if port 22 is open:"
            nc -zv ${{ needs.provision-infrastructure.outputs.instance_ip }} 22
            
            # Get system log again for debugging
            echo "Retrieving system log for debugging:"
            INSTANCE_ID=${{ needs.provision-infrastructure.outputs.instance_id }}
            if [ -n "$INSTANCE_ID" ] && [ "$INSTANCE_ID" != "None" ]; then
              aws ec2 get-console-output --instance-id $INSTANCE_ID --region ${{ env.AWS_REGION }} || echo "Could not get console output"
            fi
            
            exit 1
          fi

      - name: Create deployment directory
        run: |
          ssh ubuntu@${{ needs.provision-infrastructure.outputs.instance_ip }} 'mkdir -p ~/pokedex-app'

      - name: Copy configuration files
        run: |
          # Use explicit paths based on your repository structure
          echo "Copying docker-compose.yml from repository root"
          scp ./docker-compose.yml ubuntu@${{ needs.provision-infrastructure.outputs.instance_ip }}:~/pokedex-app/

          echo "Copying nginx.conf from frontend directory"
          scp ./frontend/nginx.conf ubuntu@${{ needs.provision-infrastructure.outputs.instance_ip }}:~/pokedex-app/

      # Modified deployment script to properly handle environment variables and ensure Docker Compose runs
      - name: Deploy with Docker Compose
        run: |
          ssh ubuntu@${{ needs.provision-infrastructure.outputs.instance_ip }} "
            cd ~/pokedex-app
            
            # Create a clean .env file directly on the server
            cat > .env << EOF
            DB_USER=${{ secrets.DB_USER }}
            DB_PASS=${{ secrets.DB_PASS }}
            DB_NAME=${{ secrets.DB_NAME }}
            API_URL=http://${{ needs.provision-infrastructure.outputs.instance_ip }}:3000
            DJANGO_SECRET_KEY=${{ secrets.DJANGO_SECRET_KEY }}
            NODE_ENV=production
            DOCKER_HUB_USERNAME=${{ secrets.DOCKER_HUB_USERNAME }}
            EOF
            
            # Print environment file existence
            echo 'Environment file created:'
            ls -la .env
            
            # Ensure Docker is installed and running
            if ! command -v docker &> /dev/null; then
              echo 'Docker not found, installing...'
              sudo apt-get update
              sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common
              curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
              sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \$(lsb_release -cs) stable\"
              sudo apt-get update
              sudo apt-get install -y docker-ce docker-ce-cli containerd.io
              sudo usermod -aG docker \$USER
            fi
            
            # Start Docker service if not running
            if ! sudo systemctl is-active --quiet docker; then
              echo 'Starting Docker service...'
              sudo systemctl start docker
              sudo systemctl enable docker
            fi
            
            # Ensure Docker Compose is installed
            if ! command -v docker-compose &> /dev/null; then
              echo 'Docker Compose not found, installing...'
              sudo curl -L \"https://github.com/docker/compose/releases/download/v2.20.0/docker-compose-\$(uname -s)-\$(uname -m)\" -o /usr/local/bin/docker-compose
              sudo chmod +x /usr/local/bin/docker-compose
            fi
            
            # Verify docker-compose.yml exists
            if [ ! -f docker-compose.yml ]; then
              echo 'ERROR: docker-compose.yml not found!'
              ls -la
              exit 1
            fi
            
            # Display docker-compose.yml content
            echo 'docker-compose.yml content:'
            cat docker-compose.yml
            
            # Verify .env exists
            if [ ! -f .env ]; then
              echo 'ERROR: .env file not found!'
              exit 1
            fi
            
            # Stop any existing containers to avoid conflicts
            echo 'Stopping any existing containers...'
            sudo docker-compose down || true
            
            # Pull latest images
            echo 'Pulling latest images...'
            sudo docker-compose pull
            
            # Start containers with force-recreate to ensure fresh start
            echo 'Starting containers...'
            sudo docker-compose up -d --force-recreate
            
            # Check if containers are running
            echo 'Container status:'
            sudo docker-compose ps
            
            # Show logs for troubleshooting
            echo 'Frontend container logs:'
            sudo docker-compose logs --tail 20 frontend || echo 'No frontend logs available'
            
            echo 'Backend container logs:'
            sudo docker-compose logs --tail 20 backend || echo 'No backend logs available'
            
            # Clean up old images
            echo 'Cleaning up old images...'
            sudo docker image prune -f --filter \"until=24h\"
          "

  # 5. Health Check and Monitoring
  health-check:
    name: Health Check and Monitoring
    needs: [provision-infrastructure, deploy-application]
    runs-on: ubuntu-latest
    steps:
      - name: Setup SSH with ssh-agent for monitoring
        run: |
          # Start ssh-agent in the background
          eval "$(ssh-agent -s)"

          # Add SSH private key to ssh-agent
          echo "${{ secrets.SSH_PRIVATE_KEY }}" | tr -d '\r' | ssh-add -

          # Add host key to known hosts
          mkdir -p ~/.ssh
          ssh-keyscan -H ${{ needs.provision-infrastructure.outputs.instance_ip }} >> ~/.ssh/known_hosts
          chmod 644 ~/.ssh/known_hosts

      - name: Check frontend availability
        id: frontend-health
        uses: jtalk/url-health-check-action@v3
        with:
          url: http://${{ needs.provision-infrastructure.outputs.instance_ip }}
          max-attempts: 8
          retry-delay: 20s
          follow-redirect: true

      - name: Check backend API availability
        id: backend-health
        uses: jtalk/url-health-check-action@v3
        with:
          url:
            http://${{ needs.provision-infrastructure.outputs.instance_ip
            }}:3000/admin/
          max-attempts: 8
          retry-delay: 20s
          follow-redirect: true

      - name: Set up monitoring
        run: |
          echo "Setting up monitoring on the server..."
          ssh ubuntu@${{ needs.provision-infrastructure.outputs.instance_ip }} '
            # Check if node exporter is installed
            if ! command -v prometheus-node-exporter &> /dev/null; then
              # Install monitoring tools
              sudo apt-get update
              sudo apt-get install -y prometheus-node-exporter
              
              # Start monitoring services
              sudo systemctl enable prometheus-node-exporter
              sudo systemctl start prometheus-node-exporter
              
              echo "Monitoring tools installed and started"
            else
              echo "Monitoring tools already installed"
            fi
            
            # Verify monitoring service is running
            if systemctl is-active --quiet prometheus-node-exporter; then
              echo "Monitoring service is running"
            else
              echo "Monitoring service is not running, attempting to start..."
              sudo systemctl start prometheus-node-exporter
            fi
          '
        continue-on-error: true

      - name: Generate deployment report
        run: |
          echo "deployment_success=true" >> $GITHUB_OUTPUT
          echo "Deployment completed successfully!"
          echo "Application URL: http://${{ needs.provision-infrastructure.outputs.instance_ip }}"
          echo "Backend API URL: http://${{ needs.provision-infrastructure.outputs.instance_ip }}:3000"
        if:
          steps.frontend-health.outcome == 'success' ||
          steps.backend-health.outcome == 'success'

      - name: Deployment failure notification
        run: |
          echo "deployment_success=false" >> $GITHUB_OUTPUT
          echo "Deployment health checks failed!"
          echo "Please check the logs for more information."
          exit 1
        if:
          steps.frontend-health.outcome != 'success' &&
          steps.backend-health.outcome != 'success'
