name: Pokedex CI/CD Pipeline

on:
  push:
    branches: [main]
  workflow_dispatch:

env:
  FRONTEND_IMAGE_NAME: pokedex-frontend
  BACKEND_IMAGE_NAME: pokedex-backend
  DOCKER_REGISTRY: docker.io
  AWS_REGION: us-east-1
  SSH_KEY_PATH: ~/.ssh/terraform-ec2

jobs:
  # 0. Prepare SSH Keys
  prepare:
    name: Prepare SSH Keys
    runs-on: ubuntu-latest
    steps:
      - name: Setup SSH key for debugging
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ${{ env.SSH_KEY_PATH }}
          chmod 600 ${{ env.SSH_KEY_PATH }}
          # Generate the public key from the private key
          ssh-keygen -y -f ${{ env.SSH_KEY_PATH }} > ${{ env.SSH_KEY_PATH }}.pub

          # Debug output - show key fingerprints (safe to display)
          echo "Private key fingerprint:"
          ssh-keygen -l -f ${{ env.SSH_KEY_PATH }}

          echo "Public key fingerprint:"
          ssh-keygen -l -f ${{ env.SSH_KEY_PATH }}.pub

          # Check key format - show first and last line only (safe to display)
          echo "Private key format check (first line):"
          head -n 1 ${{ env.SSH_KEY_PATH }}

          echo "Private key format check (last line):"
          tail -n 1 ${{ env.SSH_KEY_PATH }}

          # Verify public key format
          echo "Public key content:"
          cat ${{ env.SSH_KEY_PATH }}.pub

          echo "SSH key created successfully at ${{ env.SSH_KEY_PATH }}"

  # 1. Test and Analyze Code
  test:
    name: Test Application
    runs-on: ubuntu-latest
    needs: prepare

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      # Frontend Tests
      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: '**/package-lock.json'

      - name: Install frontend dependencies
        run: |
          cd frontend
          npm ci

      - name: Run frontend linting
        run: |
          cd frontend
          npm run lint
        continue-on-error: true

      # Backend Tests
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install backend dependencies
        run: |
          cd backend
          pip install -r requirements.txt

      - name: Run backend tests
        run: |
          cd backend
          python manage.py test
        env:
          DEBUG: 'True'
          DB_HOST: 'localhost'
          DB_NAME: 'test_db'
          DB_USER: 'postgres'
          DB_PASS: 'postgres'
          SECRET_KEY: 'test-key-for-ci'

  # 2. Build Docker Images
  build:
    name: Build Docker Images
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Login to DockerHub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKER_HUB_USERNAME }}
          password: ${{ secrets.DOCKER_HUB_TOKEN }}

      - name: Build and push frontend image
        uses: docker/build-push-action@v4
        with:
          context: ./frontend
          push: true
          tags: ${{ secrets.DOCKER_HUB_USERNAME }}/pokedex-frontend:latest
          cache-from:
            type=registry,ref=${{ secrets.DOCKER_HUB_USERNAME
            }}/pokedex-frontend:buildcache
          cache-to:
            type=registry,ref=${{ secrets.DOCKER_HUB_USERNAME
            }}/pokedex-frontend:buildcache,mode=max

      - name: Build and push backend image
        uses: docker/build-push-action@v4
        with:
          context: ./backend
          push: true
          tags: ${{ secrets.DOCKER_HUB_USERNAME }}/pokedex-backend:latest
          cache-from:
            type=registry,ref=${{ secrets.DOCKER_HUB_USERNAME
            }}/pokedex-backend:buildcache
          cache-to:
            type=registry,ref=${{ secrets.DOCKER_HUB_USERNAME
            }}/pokedex-backend:buildcache,mode=max

  # 3. Provision Infrastructure with Terraform
  provision-infrastructure:
    name: Provision Infrastructure
    needs: [prepare, build]
    runs-on: ubuntu-latest
    outputs:
      instance_ip: ${{ steps.extract-ip.outputs.instance_public_ip }}
      instance_id: ${{ steps.extract-instance-id.outputs.instance_id }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Test AWS Credentials
        run: |
          aws sts get-caller-identity
          aws ec2 describe-regions --output json | jq -r '.Regions[0].RegionName' || echo "Region listing failed"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.7

      - name: Setup SSH key for Terraform
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ${{ env.SSH_KEY_PATH }}
          chmod 600 ${{ env.SSH_KEY_PATH }}
          ssh-keygen -y -f ${{ env.SSH_KEY_PATH }} > ${{ env.SSH_KEY_PATH }}.pub
          echo "Public key for EC2:"
          cat ${{ env.SSH_KEY_PATH }}.pub

      # Clean slate approach - delete all resources with same names first
      - name: Clean up existing resources
        run: |
          # Delete any instance with the tag Name=pokedex-app-server
          INSTANCE_IDS=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=pokedex-app-server" --query "Reservations[*].Instances[*].InstanceId" --output text --region ${{ env.AWS_REGION }})
          if [ ! -z "$INSTANCE_IDS" ]; then
            echo "Terminating instances: $INSTANCE_IDS"
            aws ec2 terminate-instances --instance-ids $INSTANCE_IDS --region ${{ env.AWS_REGION }}
            echo "Waiting for instances to terminate..."
            aws ec2 wait instance-terminated --instance-ids $INSTANCE_IDS --region ${{ env.AWS_REGION }}
          fi

          # Delete the key pair if it exists
          aws ec2 delete-key-pair --key-name pokedex-app-key --region ${{ env.AWS_REGION }} || true

          # Force delete security group
          SG_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=pokedex-app-sg" --query "SecurityGroups[0].GroupId" --output text --region ${{ env.AWS_REGION }} || echo "")
          if [ "$SG_ID" != "None" ] && [ "$SG_ID" != "" ]; then
            echo "Force deleting security group $SG_ID and its dependencies..."
            # Delete any EIPs associated with instances using this SG
            EIP_ALLOC_IDS=$(aws ec2 describe-network-interfaces --filters "Name=group-id,Values=$SG_ID" --query "NetworkInterfaces[*].Association.AllocationId" --output text --region ${{ env.AWS_REGION }} || echo "")
            for ALLOC_ID in $EIP_ALLOC_IDS; do
              [ ! -z "$ALLOC_ID" ] && aws ec2 release-address --allocation-id $ALLOC_ID --region ${{ env.AWS_REGION }} || true
            done
            
            # Delete any network interfaces using this SG
            INTERFACE_IDS=$(aws ec2 describe-network-interfaces --filters "Name=group-id,Values=$SG_ID" --query "NetworkInterfaces[*].NetworkInterfaceId" --output text --region ${{ env.AWS_REGION }} || echo "")
            for INTERFACE_ID in $INTERFACE_IDS; do
              [ ! -z "$INTERFACE_ID" ] && aws ec2 delete-network-interface --network-interface-id $INTERFACE_ID --region ${{ env.AWS_REGION }} || true
            done
            
            sleep 10
            aws ec2 delete-security-group --group-id $SG_ID --region ${{ env.AWS_REGION }} || true
          fi

      - name: Create terraform.tfvars file
        run: |
          cat > terraform.tfvars << EOF
          instance_type = "t2.micro"
          app_name = "pokedex-app"
          ssh_public_key = "$(cat ${{ env.SSH_KEY_PATH }}.pub)"
          EOF

          echo "terraform.tfvars contents:"
          cat terraform.tfvars

      - name: Terraform Init
        run: terraform init

      - name: Terraform Apply Directly
        run: |
          # Skip plan and go straight to apply for simplicity
          terraform apply -auto-approve

      # These must match the IDs used in the outputs section at the job level
      - name: Extract IP from AWS
        id: extract-ip
        run: |
          # Extract IP using AWS CLI
          INSTANCE_ID=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=pokedex-app-server" --query "Reservations[0].Instances[0].InstanceId" --output text --region ${{ env.AWS_REGION }})

          IP_ADDRESS=$(aws ec2 describe-instances --instance-ids "${INSTANCE_ID}" --query "Reservations[0].Instances[0].PublicIpAddress" --output text --region ${{ env.AWS_REGION }} || echo "")

          # If not found, look for Elastic IP
          if [ -z "$IP_ADDRESS" ] || [ "$IP_ADDRESS" == "None" ]; then
            IP_ADDRESS=$(aws ec2 describe-addresses --filters "Name=instance-id,Values=${INSTANCE_ID}" --query "Addresses[0].PublicIp" --output text --region ${{ env.AWS_REGION }} || echo "")
          fi

          echo "Found IP Address: ${IP_ADDRESS}"
          echo "instance_public_ip=${IP_ADDRESS}" >> $GITHUB_OUTPUT

      # This step ID must match the one in the outputs section
      - name: Extract Instance ID from AWS
        id: extract-instance-id
        run: |
          # Extract instance ID using AWS CLI to be more reliable
          INSTANCE_ID=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=pokedex-app-server" --query "Reservations[0].Instances[0].InstanceId" --output text --region ${{ env.AWS_REGION }})
          echo "Found Instance ID: ${INSTANCE_ID}"
          echo "instance_id=${INSTANCE_ID}" >> $GITHUB_OUTPUT

      # Simple fixed wait instead of complex checking
      - name: Wait for instance initialization
        run: |
          echo "Waiting 3 minutes for instance to initialize..."
          sleep 180
          echo "Wait complete, proceeding to next steps."

          # Print the values being passed to the next steps for debugging
          echo "Instance ID being passed: ${{ steps.extract-instance-id.outputs.instance_id }}"
          echo "Instance IP being passed: ${{ steps.extract-ip.outputs.instance_public_ip }}"

  # 4. Deploy Application
  deploy-application:
    name: Deploy Application
    needs: [prepare, provision-infrastructure]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      # Set up direct SSH connection (more reliable than ssh-agent)
      - name: Setup SSH connection
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/terraform-ec2
          chmod 600 ~/.ssh/terraform-ec2

          # Get IP address directly from previous job output
          IP="${{ needs.provision-infrastructure.outputs.instance_ip }}"
          echo "Using IP address: $IP"

          # Add IP address to /etc/hosts to avoid DNS issues
          echo "$IP ec2-instance" | sudo tee -a /etc/hosts

          # Add to known hosts using IP address directly
          ssh-keyscan -H $IP >> ~/.ssh/known_hosts

          # Test connection with verbose output and IP address
          echo "Testing SSH connection..."
          ssh -v -i ~/.ssh/terraform-ec2 -o StrictHostKeyChecking=no -o ConnectTimeout=30 ubuntu@$IP 'echo "SSH connection test"' || echo "Initial test connection failed, will retry"

      - name: Wait for SSH to be available
        run: |
          max_attempts=30
          counter=0
          echo "Waiting for SSH service to become available..."

          until ssh -i ~/.ssh/terraform-ec2 -o ConnectTimeout=5 -o StrictHostKeyChecking=no -o BatchMode=yes ubuntu@${{ needs.provision-infrastructure.outputs.instance_ip }} "echo SSH is ready" || [ $counter -eq $max_attempts ]
          do
            echo "Attempt $counter/$max_attempts: SSH not available yet, waiting..."
            sleep 10
            counter=$((counter+1))
          done

          if [ $counter -eq $max_attempts ]; then
            echo "Error: SSH did not become available in time"
            exit 1
          fi

          echo "SSH is now available!"

      - name: Create deployment directory
        run: |
          ssh -i ~/.ssh/terraform-ec2 ubuntu@${{ needs.provision-infrastructure.outputs.instance_ip }} 'mkdir -p ~/pokedex-app'

      - name: Copy configuration files
        run: |
          # Use explicit paths based on your repository structure
          echo "Copying docker-compose.yml from repository root"
          scp -i ~/.ssh/terraform-ec2 ./docker-compose.yml ubuntu@${{ needs.provision-infrastructure.outputs.instance_ip }}:~/pokedex-app/

          echo "Copying nginx.conf from frontend directory"
          scp -i ~/.ssh/terraform-ec2 ./frontend/nginx.conf ubuntu@${{ needs.provision-infrastructure.outputs.instance_ip }}:~/pokedex-app/

      # Modified deployment script to properly handle environment variables and ensure Docker Compose runs
      - name: Deploy with Docker Compose
        run: |
          ssh -i ~/.ssh/terraform-ec2 ubuntu@${{ needs.provision-infrastructure.outputs.instance_ip }} << 'ENDSSH'
            cd ~/pokedex-app
            
            # Create a clean .env file directly on the server
            cat > .env << EOF
            DB_USER=${{ secrets.DB_USER }}
            DB_PASS=${{ secrets.DB_PASS }}
            DB_NAME=${{ secrets.DB_NAME }}
            API_URL=http://${{ needs.provision-infrastructure.outputs.instance_ip }}:3000
            DJANGO_SECRET_KEY=${{ secrets.DJANGO_SECRET_KEY }}
            NODE_ENV=production
            DOCKER_HUB_USERNAME=${{ secrets.DOCKER_HUB_USERNAME }}
            EOF
            
            # Print environment file existence
            echo 'Environment file created:'
            ls -la .env
            
            # Stop any existing containers to avoid conflicts
            echo 'Stopping any existing containers...'
            sudo docker-compose down || true
            
            # Pull latest images
            echo 'Pulling latest images...'
            sudo docker-compose pull
            
            # Start containers with force-recreate to ensure fresh start
            echo 'Starting containers...'
            sudo docker-compose up -d --force-recreate
            
            # Check if containers are running
            echo 'Container status:'
            sudo docker-compose ps
            
            # Show logs for troubleshooting
            echo 'Frontend container logs:'
            sudo docker-compose logs --tail 20 frontend || echo 'No frontend logs available'
            
            echo 'Backend container logs:'
            sudo docker-compose logs --tail 20 backend || echo 'No backend logs available'
          ENDSSH

  # 5. Health Check and Monitoring
  health-check:
    name: Health Check and Monitoring
    needs: [provision-infrastructure, deploy-application]
    runs-on: ubuntu-latest
    steps:
      - name: Setup SSH connection for monitoring
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/terraform-ec2
          chmod 600 ~/.ssh/terraform-ec2
          ssh-keyscan -H ${{ needs.provision-infrastructure.outputs.instance_ip }} >> ~/.ssh/known_hosts

      - name: Check frontend availability
        id: frontend-health
        uses: jtalk/url-health-check-action@v3
        with:
          url: http://${{ needs.provision-infrastructure.outputs.instance_ip }}
          max-attempts: 8
          retry-delay: 20s
          follow-redirect: true

      - name: Check backend API availability
        id: backend-health
        uses: jtalk/url-health-check-action@v3
        with:
          url:
            http://${{ needs.provision-infrastructure.outputs.instance_ip
            }}:3000/admin/
          max-attempts: 8
          retry-delay: 20s
          follow-redirect: true

      - name: Set up monitoring
        run: |
          echo "Setting up monitoring on the server..."
          ssh -i ~/.ssh/terraform-ec2 ubuntu@${{ needs.provision-infrastructure.outputs.instance_ip }} '
            # Check if node exporter is installed
            if ! command -v prometheus-node-exporter &> /dev/null; then
              # Install monitoring tools
              sudo apt-get update
              sudo apt-get install -y prometheus-node-exporter
              
              # Start monitoring services
              sudo systemctl enable prometheus-node-exporter
              sudo systemctl start prometheus-node-exporter
              
              echo "Monitoring tools installed and started"
            else
              echo "Monitoring tools already installed"
            fi
            
            # Verify monitoring service is running
            if systemctl is-active --quiet prometheus-node-exporter; then
              echo "Monitoring service is running"
            else
              echo "Monitoring service is not running, attempting to start..."
              sudo systemctl start prometheus-node-exporter
            fi
          '
        continue-on-error: true

      - name: Generate deployment report
        run: |
          echo "deployment_success=true" >> $GITHUB_OUTPUT
          echo "Deployment completed successfully!"
          echo "Application URL: http://${{ needs.provision-infrastructure.outputs.instance_ip }}"
          echo "Backend API URL: http://${{ needs.provision-infrastructure.outputs.instance_ip }}:3000"
        if:
          steps.frontend-health.outcome == 'success' ||
          steps.backend-health.outcome == 'success'

      - name: Deployment failure notification
        run: |
          echo "deployment_success=false" >> $GITHUB_OUTPUT
          echo "Deployment health checks failed!"
          echo "Please check the logs for more information."
          exit 1
        if:
          steps.frontend-health.outcome != 'success' &&
          steps.backend-health.outcome != 'success'
